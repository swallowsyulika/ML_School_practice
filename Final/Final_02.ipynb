{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import randint\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DealMissingData(X, dfX):\n",
    "    ''' deal with missing data\n",
    "        X: data, type(numpy array)\n",
    "        dfX: same as X data, type(pandas array) '''\n",
    "    \n",
    "    print(\"----------- Start deal missing data -----------\")\n",
    "    TFarr = np.array(dfX.isna().any())\n",
    "    for index, ele in enumerate(TFarr):\n",
    "        if ele:\n",
    "            if isinstance(X[0][index], str):\n",
    "                # deal with string data\n",
    "                imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "                imputer.fit(X[:, [index]])\n",
    "                X[:, [index]] = imputer.transform(X[:, [index]])\n",
    "            else:\n",
    "                # deal with digital data\n",
    "                imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                imputer.fit(X[:, [index]])\n",
    "                X[:, [index]] = imputer.transform(X[:, [index]])\n",
    "            print(f\"column {index} have missing data, fixed!\")\n",
    "        else:\n",
    "            print(f\"column {index} not have missing data\")\n",
    "            \n",
    "    print(\"----------- End deal missing data! -----------\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeOneHot(X, pos={}):\n",
    "    ''' make one-hot  \n",
    "        X: data, type(numpy array)\n",
    "        pos: where need to onehot, type(dictionary) '''\n",
    "    \n",
    "    print(\"----------- Start onehot -----------\")\n",
    "    FeaturesNum = len(X[0])    # init\n",
    "    if bool(pos):\n",
    "        # custom onehot (onehot pos that u want to)\n",
    "        for key in pos:\n",
    "            print(f\"column {FeaturesNum-pos[key]} need to one-hot, fixed!\")\n",
    "            ct = ColumnTransformer([(key, OneHotEncoder(), [FeaturesNum-pos[key]])], remainder='passthrough')\n",
    "            NewX = ct.fit_transform(X)\n",
    "            X = NewX[:, 1:]\n",
    "            FeaturesNum = len(X[0])\n",
    "    else:\n",
    "        # auto onehot (only onehot string cols)\n",
    "        i = 0\n",
    "        while i < FeaturesNum:\n",
    "            if isinstance(X[0][i], str) or i==3:\n",
    "                print(f\"column {i}({X[0][i]}) need to one-hot, fixed!\")\n",
    "                ct = ColumnTransformer([(str(i), OneHotEncoder(), [i])], remainder='passthrough')\n",
    "                NewX = ct.fit_transform(X)[:, 1:]\n",
    "                i += len(NewX[0]) - len(X[0])\n",
    "                X = NewX\n",
    "                FeaturesNum = len(X[0])\n",
    "            i += 1\n",
    "    print(\"----------- End onehot -----------\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data_train, data_test):\n",
    "    ''' normalize data\n",
    "        data_train: training data, type(numpy array)\n",
    "        data_test: testing data, type(numpy array) '''\n",
    "    \n",
    "    print(\"----------- Start normalize -----------\")\n",
    "    sc = StandardScaler()\n",
    "    data_train = sc.fit_transform(data_train)\n",
    "    data_test = sc.transform(data_test)\n",
    "    \n",
    "    print(\"----------- End normalize -----------\")\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawRelationship(X, y, label_x, label_y):\n",
    "    for key in label_x:\n",
    "        allarr = []\n",
    "        for i in range(len(X)):\n",
    "            allarr.append(X[i][label_x[key]])\n",
    "            \n",
    "        plt.scatter(allarr, y, c=\"red\")\n",
    "        plt.xlabel(key)\n",
    "        plt.ylabel(label_y)\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawPredict(gt, pred, title):\n",
    "    ''' data [[groundTruth, predict], [], ....]'''\n",
    "    \n",
    "    data = []\n",
    "    for i in range(len(gt)):\n",
    "        data.append([gt[i], pred[i]])\n",
    "        \n",
    "    data.sort(key=lambda x:x[0])\n",
    "    for index, ele in enumerate(data):\n",
    "        plt.scatter(index, data[index][1], c=\"blue\", s=0.7)\n",
    "        plt.scatter(index, data[index][0], c=\"red\", s=0.7)\n",
    "    plt.xlabel(\"dataNums\")\n",
    "    plt.ylabel(\"charges\")\n",
    "    plt.title(title)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndTestModel(model, X_train, y_train, X_test, y_test, title):\n",
    "    print(\"Start fit data\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"fit complet\")\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    #train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    #DrawPredict(y_train, train_pred, title+\"_Train\" + \"\\n\" + f\"ACC: {train_acc}\")\n",
    "    \n",
    "    test_pred = model.predict(X_test)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    #test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    #DrawPredict(y_test, test_pred, title+\"_Test\" + \"\\n\" + f\"ACC: {test_acc}\")\n",
    "    \n",
    "    train_cm = confusion_matrix(y_train, train_pred)\n",
    "    test_cm = confusion_matrix(y_test, test_pred)\n",
    "    print(f\"Train_cm:\\n{train_cm}\")\n",
    "    print(f\"Test_cm:\\n{test_cm}\")\n",
    "    print(f\"Train acc: {train_acc}\")\n",
    "    print(f\"Test acc: {test_acc}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Start deal missing data -----------\n",
      "column 0 not have missing data\n",
      "column 1 have missing data, fixed!\n",
      "column 2 have missing data, fixed!\n",
      "column 3 have missing data, fixed!\n",
      "column 4 have missing data, fixed!\n",
      "column 5 have missing data, fixed!\n",
      "column 6 have missing data, fixed!\n",
      "column 7 have missing data, fixed!\n",
      "column 8 have missing data, fixed!\n",
      "column 9 have missing data, fixed!\n",
      "column 10 have missing data, fixed!\n",
      "column 11 have missing data, fixed!\n",
      "column 12 have missing data, fixed!\n",
      "column 13 have missing data, fixed!\n",
      "column 14 have missing data, fixed!\n",
      "column 15 have missing data, fixed!\n",
      "column 16 have missing data, fixed!\n",
      "column 17 have missing data, fixed!\n",
      "column 18 have missing data, fixed!\n",
      "column 19 have missing data, fixed!\n",
      "column 20 have missing data, fixed!\n",
      "----------- End deal missing data! -----------\n",
      "----------- Start deal missing data -----------\n",
      "column 0 not have missing data\n",
      "----------- End deal missing data! -----------\n",
      "['Albury' 13.4 22.9 0.6 5.469824216349109 7.624853113193594 'W' 44.0 'W'\n",
      " 'WNW' 20.0 24.0 71.0 22.0 1007.7 1007.1 8.0 4.503166899728551 16.9 21.8\n",
      " 'No']\n",
      "['No']\n",
      "----------- Start onehot -----------\n",
      "column 0(Albury) need to one-hot, fixed!\n",
      "column 53(W) need to one-hot, fixed!\n",
      "column 69(W) need to one-hot, fixed!\n",
      "column 84(WNW) need to one-hot, fixed!\n",
      "column 109(No) need to one-hot, fixed!\n",
      "----------- End onehot -----------\n",
      "----------- Start onehot -----------\n",
      "column 0(No) need to one-hot, fixed!\n",
      "----------- End onehot -----------\n",
      "----------- Start normalize -----------\n",
      "----------- End normalize -----------\n",
      "[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 13.4 22.9 0.6 5.469824216349109 7.624853113193594 44.0\n",
      " 20.0 24.0 71.0 22.0 1007.7 1007.1 8.0 4.503166899728551 16.9 21.8]\n",
      "0.0\n",
      "Preprocessing data done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./DATA/final_project_dataset_2.csv\")\n",
    "\n",
    "dfX = df.iloc[:, 1:-1]\n",
    "X = dfX.values\n",
    "dfy = df.iloc[:, [-1]]\n",
    "y = dfy.values\n",
    "'''\n",
    "label_x = {'Location': 0, 'MinTemp': 1, 'MaxTemp': 2,\n",
    "           'Rainfall': 3, 'Evaporation': 4, 'Sunshine': 5,\n",
    "           'WindGustDir': 6, 'WindGustSpeed': 7, 'WindDir9am': 8,\n",
    "           'WindDir3pm': 9, 'WindSpeed9am': 10, 'WindSpeed3pm': 11,\n",
    "           'Humidity9am': 12, 'Humidity3pm': 13, 'Pressure9am': 14,\n",
    "           'Pressure3pm': 15, 'Cloud9am': 16, 'Cloud3pm': 17,\n",
    "           'Temp9am': 18, 'Temp3pm': 19, 'RainToday': 20\n",
    "          }\n",
    "'''\n",
    "#DrawRelationship(X, y, label_x=label_x, label_y='RainTomorrow')\n",
    "\n",
    "X = DealMissingData(X, dfX)\n",
    "y = DealMissingData(y, dfy)\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "#pos2onehot = {'Sex': 5, 'children': 3, 'Smoker': 2, 'Region': 1}\n",
    "X = MakeOneHot(X)\n",
    "y = MakeOneHot(y)\n",
    "y = y.reshape(1, -1)[0]\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_test = NormalizeData(X_train, X_test)\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "print(\"Preprocessing data done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0516031  0.03238822 0.02460146 0.02268849 0.01912658 0.01843925\n",
      " 0.01641429 0.01622303 0.01473322 0.01395672 0.01322049 0.0127462\n",
      " 0.0122484  0.01194041 0.01174589 0.01125593 0.01115764 0.01104661\n",
      " 0.01090434 0.01084147 0.01055176 0.01044841 0.01034639 0.01022954\n",
      " 0.01016913 0.01004155 0.00998548 0.009939   0.00988138 0.0097174\n",
      " 0.009696   0.00967766 0.00956333 0.00952121 0.00950352 0.00940687\n",
      " 0.00939783 0.00932326 0.00931201 0.00926655 0.00923699 0.00922514\n",
      " 0.0091603  0.00911909 0.00908789 0.0090626  0.00899397 0.00897306\n",
      " 0.00891726 0.00888736]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=50)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2020-12-31 16:12:13.564799\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fit data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit complet\n",
      "Train_cm:\n",
      "[[83439  4810]\n",
      " [12481 13024]]\n",
      "Test_cm:\n",
      "[[20887  1180]\n",
      " [ 3102  3270]]\n",
      "Train acc: 0.847996553967333\n",
      "Test acc: 0.8494321178663103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TrainAndTestModel(SVC(), X_train, y_train, X_test, y_test, \"SVC\")\n",
    "TrainAndTestModel(LogisticRegression(verbose=True), X_train, y_train, X_test, y_test, \"Logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2020-12-31 16:12:14.888236\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fit data\n",
      "fit complet\n",
      "Train_cm:\n",
      "[[88248     1]\n",
      " [    3 25502]]\n",
      "Test_cm:\n",
      "[[19119  2948]\n",
      " [ 2891  3481]]\n",
      "Train acc: 0.9999648364013574\n",
      "Test acc: 0.79468335736137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainAndTestModel(DecisionTreeClassifier(), X_train, y_train, X_test, y_test, \"DecisionTree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fit data\n",
      "fit complet\n",
      "Train_cm:\n",
      "[[88248     1]\n",
      " [    4 25501]]\n",
      "Test_cm:\n",
      "[[21219   848]\n",
      " [ 3160  3212]]\n",
      "Train acc: 0.9999560455016966\n",
      "Test acc: 0.8590667744998066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainAndTestModel(RandomForestClassifier(), X_train, y_train, X_test, y_test, \"RandomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
