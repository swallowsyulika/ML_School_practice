{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".\\DATA\\house.csv\")\n",
    "\n",
    "X = df.iloc[:, [2, 3, 4, 5, 6, 7, 9]].values\n",
    "y = df.iloc[:, 8].values\n",
    "\n",
    "# one-hot\n",
    "ct = ColumnTransformer([(\"ocean_proximity\", OneHotEncoder(), [6])], remainder='passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# one-hot drop one column\n",
    "X = X[:, 1:]\n",
    "\n",
    "# deal with missing data\n",
    "Imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "Imputer.fit(X[:, :])\n",
    "X[:, :] = Imputer.transform(X[:, :])\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try linearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6277272869749327\n"
     ]
    }
   ],
   "source": [
    "# print acc\n",
    "acc = regressor.score(X_test, y_test)\n",
    "print(f\"acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add const\n",
    "X_train = np.append(arr=np.ones((len(X_train), 1)).astype(int), values=X_train, axis=1)\n",
    "# try to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.635</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.635</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   2873.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 03 Dec 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:26:45</td>     <th>  Log-Likelihood:    </th> <td>-2.0761e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16512</td>      <th>  AIC:               </th>  <td>4.152e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16501</td>      <th>  BIC:               </th>  <td>4.153e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 2.715e+04</td> <td> 2735.398</td> <td>    9.925</td> <td> 0.000</td> <td> 2.18e+04</td> <td> 3.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-6.736e+04</td> <td> 1404.760</td> <td>  -47.952</td> <td> 0.000</td> <td>-7.01e+04</td> <td>-6.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1.818e+05</td> <td> 4.04e+04</td> <td>    4.504</td> <td> 0.000</td> <td> 1.03e+05</td> <td> 2.61e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 4066.8027</td> <td> 1890.666</td> <td>    2.151</td> <td> 0.031</td> <td>  360.895</td> <td> 7772.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> 1.449e+04</td> <td> 1725.734</td> <td>    8.397</td> <td> 0.000</td> <td> 1.11e+04</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> 1188.0906</td> <td>   49.437</td> <td>   24.032</td> <td> 0.000</td> <td> 1091.189</td> <td> 1284.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -5.8469</td> <td>    0.881</td> <td>   -6.637</td> <td> 0.000</td> <td>   -7.574</td> <td>   -4.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   53.5321</td> <td>    6.656</td> <td>    8.042</td> <td> 0.000</td> <td>   40.485</td> <td>   66.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>  -37.5084</td> <td>    1.185</td> <td>  -31.654</td> <td> 0.000</td> <td>  -39.831</td> <td>  -35.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>  100.3559</td> <td>    7.405</td> <td>   13.552</td> <td> 0.000</td> <td>   85.841</td> <td>  114.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>     4e+04</td> <td>  373.717</td> <td>  107.043</td> <td> 0.000</td> <td> 3.93e+04</td> <td> 4.07e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3923.937</td> <th>  Durbin-Watson:     </th> <td>   1.998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>13986.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.169</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 6.855</td>  <th>  Cond. No.          </th> <td>2.93e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.93e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.635\n",
       "Model:                            OLS   Adj. R-squared:                  0.635\n",
       "Method:                 Least Squares   F-statistic:                     2873.\n",
       "Date:                Thu, 03 Dec 2020   Prob (F-statistic):               0.00\n",
       "Time:                        09:26:45   Log-Likelihood:            -2.0761e+05\n",
       "No. Observations:               16512   AIC:                         4.152e+05\n",
       "Df Residuals:                   16501   BIC:                         4.153e+05\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       2.715e+04   2735.398      9.925      0.000    2.18e+04    3.25e+04\n",
       "x1         -6.736e+04   1404.760    -47.952      0.000   -7.01e+04   -6.46e+04\n",
       "x2          1.818e+05   4.04e+04      4.504      0.000    1.03e+05    2.61e+05\n",
       "x3          4066.8027   1890.666      2.151      0.031     360.895    7772.711\n",
       "x4          1.449e+04   1725.734      8.397      0.000    1.11e+04    1.79e+04\n",
       "x5          1188.0906     49.437     24.032      0.000    1091.189    1284.993\n",
       "x6            -5.8469      0.881     -6.637      0.000      -7.574      -4.120\n",
       "x7            53.5321      6.656      8.042      0.000      40.485      66.579\n",
       "x8           -37.5084      1.185    -31.654      0.000     -39.831     -35.186\n",
       "x9           100.3559      7.405     13.552      0.000      85.841     114.871\n",
       "x10             4e+04    373.717    107.043      0.000    3.93e+04    4.07e+04\n",
       "==============================================================================\n",
       "Omnibus:                     3923.937   Durbin-Watson:                   1.998\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            13986.694\n",
       "Skew:                           1.169   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.855   Cond. No.                     2.93e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.93e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "X_opt = X_train[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "X_opt = np.array(X_opt, dtype=float)\n",
    "regressor_OLS = sm.OLS(endog=y_train, exog=X_opt).fit()\n",
    "regressor_OLS.summary()    # done!\n",
    "# all P < 0.05\n",
    "# it doesn't need optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
